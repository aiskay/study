{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 特徴選択および特徴量の重要度\n",
    "\n",
    "モデルの予測に寄与しない特徴を判定することで、それらを取り除き精度向上や計算時間の短縮が可能となる。\n",
    "\n",
    "ここでは、以下の 3 つの方法を用いてそのような特徴量の判定方法を説明する。\n",
    "\n",
    "1. 単変量統計を用いる方法\n",
    "2. 特徴量の重要度を用いる方法\n",
    "3. 反復して探索する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_x は学習データ、train_y は目的変数、test_x はテストデータ\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed_onehot.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 単変量統計を用いる方法\n",
    "\n",
    "各特徴量と目的変数から何らかの統計量を計算し、その統計量の順序で特徴量を選択することを考える。\n",
    "\n",
    "その中で、特徴量と目的変数の 1:1 の関係に着目した単変量統計について考える。\n",
    "\n",
    "### 相関係数\n",
    "\n",
    "各特徴量 $x_i$ と目的変数 $y_i$ の相関係数 (ピアソンの積率相関係数)\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - {x})^2\\sum_i(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "を計算してその絶対値の大きい順に選択する。  \n",
    "線形以外の関係性を捉えることはできないので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# 相関係数\n",
    "# ---------------------------------\n",
    "import scipy.stats as st\n",
    "\n",
    "# 相関係数\n",
    "corrs = []\n",
    "for c in train_x.columns:\n",
    "    corr = np.corrcoef(train_x[c], train_y)[0, 1]\n",
    "    corrs.append(corr)\n",
    "corrs = np.array(corrs)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "# np.argsortを使うことで、値の順序のとおりに並べたインデックスを取得できる\n",
    "idx = np.argsort(np.abs(corrs))[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もう一つ相関を計る統計量として、スピアマンの順位相関係数がある。  \n",
    "ピアソンの相関係数では特徴量と考えていた $x_i, y_i$ を何らかの指標に基づいた単なる順位と置き換えてあげて\n",
    "\n",
    "$$\n",
    "\\sum_i x_i = \\sum_i y_i = \\frac{n(n+1)}{2}, \\quad \n",
    "\\sum_i x_i^2 = \\sum_i y_i^2 = \\frac{n(n+1)(2n+1)}{6},\n",
    "$$\n",
    "\n",
    "などとしてあげると、ピアソンの相関係数はそのまま以下のように変形できる。\n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6}{n(n^2-1)} \\sum_{i=1}^n (x_i - y_i)^2\n",
    "$$\n",
    "\n",
    "これをスピアマンの順位相関係数と呼び、 例えば生徒の英語の成績の順位と数学の成績の順位に相関があるかなどを計ったりする。  \n",
    "計算してみると、順位が全て同じときは $1$ に、逆に順位が真逆のときは $-1$ になることがわかる。\n",
    "\n",
    "同率の順位のものが多くある場合はもう少し複雑な式が用いられる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スピアマンの順位相関係数\n",
    "corrs_sp = []\n",
    "for c in train_x.columns:\n",
    "    corr_sp = st.spearmanr(train_x[c], train_y).correlation\n",
    "    corrs_sp.append(corr_sp)\n",
    "corrs_sp = np.array(corrs_sp)\n",
    "\n",
    "idx2 = np.argsort(np.abs(corrs_sp))[::-1]\n",
    "top_cols2, top_importances2 = train_x.columns.values[idx][:5], corrs_sp[idx][:5]\n",
    "print(top_cols2, top_importances2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カイ二乗統計量\n",
    "\n",
    "各特徴量と目的変数について独立性の検定を行い、カイ二乗検定の統計量を計算する。  \n",
    "\n",
    "$i$ 番目の特徴量について、target が $j$ である $k (k=1, 2, ..., n)$ 番目のデータを $f_{ijk}$ として、観測度数 $O$ と期待度数 $E$ を定義する。\n",
    "\n",
    "$$\n",
    "O_{ij} = \\sum_{k}^n f_{ijk}, \\quad E_{ij} = \\frac{({\\rm number\\ of\\ data\\ with\\ target}\\ j)}{n}.\n",
    "$$\n",
    "\n",
    "もし $i$ 番目の特徴量と target が独立であれば、以下の統計量は自由度 $n-1$ の $\\chi^2$ 分布に従うはずである。\n",
    "\n",
    "$$\n",
    "\\chi^2_i \\equiv \\sum_j \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\sim \\chi^2(n-1).\n",
    "$$\n",
    "\n",
    "従って、以上の統計量を求めることで、独立かどうかの判定を行うことができる。\n",
    "\n",
    "- 特徴量の値でスケールされるので、min-max scaling などを事前に行う\n",
    "- 独立性の検定は一般に頻度を表す特徴量について用いられるが、機械学習の文脈だと連続値に対しても応用されている  \n",
    "  従って、分類タスクでの非負の特徴量についてのみ使える\n",
    "\n",
    "python では `sklearn.feature_selection` を使えば簡単に実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# カイ二乗統計量\n",
    "x = MinMaxScaler().fit_transform(train_x)\n",
    "c2, _ = chi2(x, train_y)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "idx = np.argsort(c2)[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相互情報量\n",
    "\n",
    "各特徴量と目的変数の相互情報量\n",
    "\n",
    "$$\n",
    "I(X; Y) = \\int_X \\int_Y p(x, y) \\log \\frac{p(x,y)}{p(x)p(y)} dxdy\n",
    "$$\n",
    "\n",
    "を計算し、大きいものから特徴量を選択する。\n",
    "\n",
    "一般にデータから (同時) 確率分布を求めるのはそんなに簡単ではないと思うが、やり方は [sklearn の reference](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) を参照してみてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# 相互情報量\n",
    "# ---------------------------------\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# 相互情報量\n",
    "mi = mutual_info_classif(train_x, train_y)\n",
    "\n",
    "# 重要度の上位を出力する（上位5個まで）\n",
    "idx = np.argsort(mi)[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], corrs[idx][:5]\n",
    "print(top_cols, top_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、学習データ全体で特徴量選択をしてしまうと、本来は目的変数と関係ないにもかかわらずたまたま学習データで偏りが出ている特徴量が有効な特徴量と認識されてしまうことがある。  \n",
    "そのため、特徴量選択も out-of-fold で行ったほうがよいいこともあると覚えておくとよい。\n",
    "\n",
    "## 6.2.2 特徴量の重要度を用いる方法\n",
    "\n",
    "モデルから出力される特徴量の重要度を用いて特徴量を選択する方法を紹介する。\n",
    "\n",
    "### ランダムフォレストの特徴量の重要度\n",
    "\n",
    "Random forest の重要度は、分岐を作成するときの基準となる値の減少によって計算される。  \n",
    "回帰では二乗誤差、分類ではジニ不純度 \n",
    "$$\n",
    "\\sum_i^{n} p_i (1-p_i)\n",
    "$$\n",
    "($p_i$ はあるノードにおけるターゲットラベル $i$ の頻度)\n",
    "を元にして計算される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# ランダムフォレストの特徴量の重要度\n",
    "# ---------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ランダムフォレスト\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=71)\n",
    "clf.fit(train_x, train_y)\n",
    "fi = clf.feature_importances_\n",
    "\n",
    "# 重要度の上位を出力する\n",
    "idx = np.argsort(fi)[::-1]\n",
    "top_cols, top_importances = train_x.columns.values[idx][:5], fi[idx][:5]\n",
    "print('random forest importance')\n",
    "print(top_cols, top_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT の特徴量の重要度\n",
    "\n",
    "xgboost では以下の 3 つの種類の特徴量の重要度を出力できる。\n",
    "\n",
    "- ゲイン: その特徴量の分岐により得た目的関数の減少\n",
    "- カバー: その特徴量により分岐させられたデータの数\n",
    "- 頻度: その特徴量が分岐に現れた回数\n",
    "\n",
    "python の default では頻度が出力されるが、ゲインを出力したほうが良い。\n",
    "\n",
    "連続変数やカテゴリの多いカテゴリ変数は分岐の候補が多いため上位になりやすかったりする。  \n",
    "そのため、バラつきを考慮したりランダムな値からなる特徴量と比較することが有効。  \n",
    "例えば、cross validation の fold 間での標準偏差/平均を計算し、変動係数が小さい順に特徴量を選択する手法がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# xgboostの特徴量の重要度\n",
    "# ---------------------------------\n",
    "import xgboost as xgb\n",
    "\n",
    "# xgboost\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 71}\n",
    "num_round = 50\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# 重要度の上位を出力する\n",
    "fscore = model.get_score(importance_type='total_gain')  # or 'total_cover'\n",
    "fscore = sorted([(k, v) for k, v in fscore.items()], key=lambda tpl: tpl[1], reverse=True)\n",
    "print('xgboost importance')\n",
    "print(fscore[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### その他の手法\n",
    "\n",
    "#### permutation importance\n",
    "\n",
    "モデルを学習した後、validation data のある特徴量をシャッフルした場合の予測とシャッフルしていない予測を比較し、予測精度の落ち具合から特徴量の重要度を判定する方法。  \n",
    "`eli4` というライブラリが使える他、random forest の場合、学習データのサンプリングから外れた out-of-bag と呼ばれるデータを用いて `rfpimp` などのモジュールにより計算することができる。\n",
    "\n",
    "#### null importance\n",
    "\n",
    "目的変数をシャッフルして学習させた場合のモデルの重要度を null importance として基準とし、目的変数をシャッフルさせていない通常の重要度と比較して特徴量を選択する手法。  \n",
    "null importance はシャッフルによって変わるため、数十回繰り返して統計量を用いる。\n",
    "\n",
    "実装例は Home Credit Default Risk の kaggle kenel [Feature Selection with Null Importances](https://www.kaggle.com/ogrellier/feature-selection-with-null-importances) を参考。\n",
    "\n",
    "#### boruta\n",
    "\n",
    "それぞれの特徴量をシャッフルしたデータ shadow feature を元のデータに加えて random forest で学習を行い、それぞれの特徴量の重要度が全ての shadow feature より大きいものを記録する。  \n",
    "これを何度か繰り返し、shadow feature より重要とは言えない特徴量を除外していく。\n",
    "\n",
    "ライブラリ [`BorutaPy`](https://danielhomola.com/boruta_py) が公開されており、実装例は kaggle kernel [Boruta feature elimination](https://www.kaggle.com/tilii6/boruta-feature-elimination)を参考のこと。\n",
    "\n",
    "#### 特徴量を大量生成してからの特徴量選択\n",
    "\n",
    "機械的に特徴量を大量生成してから特徴量選択をする手法。\n",
    "\n",
    "#### xgbfir\n",
    "\n",
    "xgboost のモデルから決定木分岐の情報を抽出して特徴量の重要度を出力するライブラリ。  \n",
    "2016 以降更新されてなさそうなので、使うことはなさそう...？\n",
    "\n",
    "## 5.2.3 反復して探索する方法\n",
    "\n",
    "特徴量の組み合わせを変えて学習を繰り返し、その精度などを用いて探索する手法。  \n",
    "時間もかかるしあまりやることはないような気がするので省略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
