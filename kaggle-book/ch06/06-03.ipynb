{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 クラスの分布が偏っている場合\n",
    "\n",
    "分類ラベルの分布が偏っている場合の対処法\n",
    "\n",
    "## アンダーサンプリング\n",
    "\n",
    "例えば異常検知など正例が異常に少ない場合、負例の一部のみを使用してモデルを学習させる方法。  \n",
    "また、異なる負例を取り出して学習させた複数のモデルを平均する手法 (バギング) も有効。  \n",
    "[`imbalanced-learn`](https://imbalanced-learn.org/stable/) というライブラリを使える。(参考: [LightGBMでdownsampling+bagging](https://upura.hatenablog.com/entry/2019/01/12/193000))\n",
    "\n",
    "kaggle の [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection) というコンペでは、1 億件以上のデータのうち正例は 0.2 % 程度であり、\n",
    "1 位の解法はほとんどの負例を捨てて精度を出していた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# データ等の準備\n",
    "# ----------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# train_x は学習データ、train_y は目的変数、test_x はテストデータ\n",
    "train = pd.read_csv('../input/sample-data/train_preprocessed_onehot.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../input/sample-data/test_preprocessed_onehot.csv')\n",
    "\n",
    "# downsampling\n",
    "# 正例の数と負例の数を揃える\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_resampled, y_resampled = sampler.fit_resample(train_x, train_y)\n",
    "print(len(train[train['target']==1])*2, len(x_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特に工夫をしない\n",
    "\n",
    "特に工夫をしなくても十分な精度が出ることも多い。\n",
    "\n",
    "## 重み付け\n",
    "\n",
    "parameter 更新の際の正例と負例の寄与の合計が等しくなるように正例に高い weight を指定する方法。\n",
    "\n",
    "## オーバーサンプリング\n",
    "\n",
    "負例の方が多い場合に、正例を増やして学習をさせる方法。  \n",
    "Synthetic Minority Oversampling Technique (SMOTE) といった手法がある。\n",
    "\n",
    "SMOTE とは、少数クラスのデータ点と近傍のデータ点間の単純な内挿によって新しいデータを生成する手法。  \n",
    "拡張版が色々ある。\n",
    "\n",
    "## 確率を予測する必要がある場合の注意点\n",
    "\n",
    "評価指標が logloss などで適切な確率を予測する必要がある場合、正例と負例の比率を変えた場合は確率の補正を忘れずに行うように。  \n",
    "(2.5.4 確率の予測値とその調整 を参照)\n",
    "\n",
    "分析コンペでは under sampling or 何もしない手法が主に使われており、over sampling はあまり使われることはない。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
